# Structured API

# DataFrame, SQL, Dataset
구조적 API에는 다음과 같은 세 가지 분산 컬렉션 API가 있습니다.
Dataset, DataFrame, SQL (테이블과 뷰)

스파크는 트랜스포메이션의 처리 과정을 정의하는 분산 프로그래밍 모델입니다.
사용자가 정의한 다수의 트랜스포메이션은 지향성 비순환 그래프(DAG)로 표현되는 명령을 만들어냅니다.
액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행합니다.

트랜스포메이션과 액션으로 다루는 논리적 구조가 바로 DataFrame과 Dataset입니다.
새로운 DataFrame이나 Dataset을 만들려면 트랜스포메이션을 호출해야 합니다.
연산을 시작하거나 데이터 타입으로 변환하려면 액션을 호출해야 합니다.

## RDD(Resilient Distributed Dataset)
RDD는 스파크 1.x 버전의 핵심 API입니다. 스파크 2.x 버전에서도 사용할 수 있지만 잘 사용하지 않습니다.
스파크에서는 기본적으로 Dataframe을 사용하는게 좋지만, 비정형 데이터나 정제되지 않은 원시 데이터를 처리해야 할 때는 RDD를 사용해야 할 수도 있습니다.

RDD는 불변성을 가지며 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음입니다.
DataFrame의 각 레코드는 스키마를 알고 있는 필드로 구성된 구조화된 로우인 반면,
RDD의 레코드는 그저 프로그래머가 선택하는 자바, 스칼라, 파이썬의 객체입니다.

사용자가 실행하는 모든 DataFrame이나 Dataset 코드는 RDD로 컴파일됩니다.
언어와 관계없이 동일한 실행 특성을 제공하는 DataFrame API와는 다르게 RDD는 언어별로 세부 구현 방식에서 차이를 보입니다.

RDD의 모든 레코드는 자바나 파이썬의 객체이므로 완벽하게 제어할 수 있습니다.
이러한 객체에는 사용자가 원하는 포맷을 사용해 원하는 모든 데이터를 저장할 수 있습니다.
개발자는 강력한 제어권을 가질 수 있지만, 잠재적인 문제가 발생할 수 있습니다.
모든 값을 다루거나, 값 사이의 상호작용 과정을 반드시 수동으로 정의해야 합니다.

구조적 API와는 다르게 레코드의 내부 구조를 스파크에서 파악할 수 없으므로 최적화를 하려면 훨씬 많은 수작업이 필요합니다.
예를 들어 스파크의 구조적 API는 자동으로 데이터를 최적화하고 압축된 바이너리 포맷으로 저장합니다.
반면 저수준 API에서 동일한 공간 효율성과 성능을 얻으려면 객체에 이런 포맷 타입을 구현해 모든 저수준 연산 과정에서 사용해야 합니다.
이와 유사하게 스파크 SQL에서 자동으로 수행되는 필터 재정렬과 집계 같은 최적화 기법도 직접 구현해야 합니다.

RDD는 DataFrame API에서 최적화된 물리적 실행 계획을 만드는 데 대부분 사용됩니다.

스칼라와 자바를 사용하는 경우에는 대부분 비슷한 성능이 나오지만, 원형 객체를 다룰 때는 큰 성능 손실이 발생할 수 있습니다.
반면 파이썬을 사용해 RDD를 다룰 때는 상당한 성능 저하가 발생할 수 있습니다.
파이썬으로 RDD를 실행하는 것은 파이썬으로 만들어진 사용자 정의 함수를 사용해 로우마다 적용하는 것과 동일하다고 볼 수 있습니다.

## Dataset
Dataset은 자바와 스칼라의 정적 데이터 타입에 맞는 코드, 즉 정적 타입 코드(statically typed code)를 지원하기 위해 고안된 스파크의 구조적 API입니다.
동적 타입 언어인 파이썬과 R에서는 사용할 수 없습니다.
스칼라에서는 케이스 클래스 객체를 사용하고, 자바에서는 자바 빈 객체를 사용해 Dataset의 각 로우를 구성하는 객체를 정의합니다.
Dataset API는 타입 안정성을 지원하므로 초기화에 사용한 클래스 대신 다른 클래스를 사용해 접근할 수 없습니다.

Dataset의 장점은 액션을 수행할 때 DataFrame을 구성하는 Row 타입의 객체가 아닌 매개변수로 지정한 타입의 객체로 반환합니다.
이 작업은 코드 변경 없이 타입 안정성을 보장할 수 있고, 스칼라의 Seq 객체 또는 자바의 ArrayList 등의 고정 타입형 컬렉션으로 다룰 수 있습니다.
하지만 사용자 정의 데이터 타입을 사용하면 성능이 나빠지게 됩니다.

도메인별 특정 객체를 효과적으로 지원하기 위해 '인코더(encoder)'라 부르는 특수 개념이 필요합니다.
인코더는 도메인별 특정 객체를 스파크의 내부 데이터 타입으로 매핑하는 시스템을 의미합니다.
인코더는 런타임 환경에서 객체를 바이너리 구조로 직렬화하는 코드를 생성하도록 스파크에 지시합니다.

Dataset을 사용하는 두 가지 이유입니다.
●    DataFrame 기능만으로는 수행할 연산을 표현할 수 없는 경우
●    성능 저하를 감수하더라도 타입 안정성(type-safe)을 가진 데이터 타입을 사용하고 싶은 경우

전체 로우에 대한 다양한 트랜스포메이션을 재사용하려면 Dataset을 사용하는 것이 적합합니다.
스파크의 API는 스칼라 Sequence 타입의 API가 분산 방식으로 동작한다는 것을 알 수 있을 겁니다.

트랜스포메이션의 첫 번째 단계에서 필터링 전에 로우 단위로 데이터를 파싱하거나,
드라이버로 데이터를 수집해 단일 노드의 라이브러리로 수집된 데이터를 처리하는 트랜스포메이션의 마지막 단계에서 사용할 수 있습니다.

## Java: Encoders
데이터 타입 클래스를 정의한 다음 DataFrame(Dataset<Row> 타입)에 지정해 인코딩할 수 있습니다.
```java
  import org.apache.spark.sql.Encoders;
  public class MyDataClass implements Serializable{
    String Column1;
  }

  Dataset<MyDataClass> ds = spark.read.parquet("/path/to/parquet")
    .as(Encoders.bean(MyDataClass.class));
```

## Scala: Case Class
케이스 클래스(case class) 구문을 사용해 데이터 타입을 정의해야 합니다.
케이스 클래스는 다음과 같은 특징을 가진 정규 클래스(regular class)입니다.
●    불변성
●    패턴 매칭으로 분해 가능
●    참조값 대신 클래스 구조를 기반으로 비교
●    사용하기 쉽고 다루기 편함
```scala
  case class MyDataClass(Column1: String)

  val myDF = spark.read.parquet("/path/to/parquet")

  val ds = myDF.as[MyDataClass]
```

## SparkSQL
스파크 SQL은 DataFrame과 Dataset API에 통합되어 있습니다. 따라서 SQL과 DataFrame의 두 방식 모두 동일한 실행 코드로 컴파일됩니다.
스파크 2.0 버전에는 하이브를 지원할 수 있는 상위 호환 기능으로 ANSI-SQL과 HiveQL을 모두 지원하는 자체 개발된 SQL 파서가 포함되어 있습니다.

## Spark 와 Hive
스파크 SQL은 하이브 메타스토어를 사용하므로 하이브와 잘 연동할 수 있습니다.
하이브 메타스토어는 여러 세션에서 사용할 테이블 정보를 보관하고 있습니다.
스파크 SQL은 하이브 메타스토어에 접속(이미 하이브를 사용하고 있는 경우)한 뒤 조회할 파일 수를 최소화하기 위해 메타데이터를 참조합니다.

## Catalog
스파크 SQL에서 가장 높은 추상화 단계는 카탈로그(catalog)입니다.
카탈로그는 테이블에 저장된 데이터에 대한 메타데이터뿐만 아니라 데이터베이스, 테이블, 함수 그리고 뷰에 대한 정보를 추상화합니다.
카탈로그는 테이블, 데이터베이스 그리고 함수를 조회하는 등 여러 가지 유용한 함수를 제공합니다.

## Table
스파크 SQL을 사용해 유용한 작업을 수행하려면 먼저 테이블을 정의해야 합니다.
테이블은 명령을 실행할 데이터의 구조라는 점에서 DataFrame과 논리적으로 동일합니다.

DataFrame은 프로그래밍 언어로 정의하지만 테이블은 데이터베이스에서 정의합니다.
스파크에서 테이블을 생성하면 default 데이터베이스에 등록됩니다.

테이블은 항상 데이터를 가지고 있다는 점을 반드시 기억해야 합니다.
임시 테이블의 개념이 없으며 데이터를 가지지 않은 뷰만 존재합니다.
테이블을 제거하면 모든 데이터가 제거되므로 조심해야 합니다.

## Spark managed table
관리형 테이블과 외부 테이블의 개념은 반드시 기억해두어야 합니다.
테이블은 테이블의 데이터와 테이블에 대한 데이터, 즉 메타데이터를 저장합니다.

DataFrame의 saveAsTable 메서드는 스파크가 관련된 모든 정보를 추적할 수 있는 관리형 테이블을 만들 수 있습니다.
디스크에 저장된 파일을 이용해 테이블을 정의하면 외부 테이블을 정의하는 겁니다.
스파크는 외부 테이블의 메타데이터를 관리합니다. 하지만 데이터 파일은 스파크에서 관리하지 않습니다.

## View
기본적으로 뷰는 단순 쿼리 실행 계획일 뿐입니다.
뷰를 사용하면 쿼리 로직을 체계화하거나 재사용하기 편하게 만들 수 있습니다.

## DataFrame과 SQL
스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰로 등록한 후 SQL 쿼리를 사용할 수 있습니다.
스파크는 SQL 쿼리를 DataFrame 코드와 같은 실행 계획으로 컴파일하므로 둘 사이의 성능 차이는 없습니다.

## DataFrame과 Dataset
DataFrame과 Dataset은 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 '실행 계획'이며, 불변성을 가집니다.

DataFrame과 Dataset은 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션입니다.
각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야 합니다('값 없음'은 null로 표시합니다).
그리고 컬렉션의 모든 로우는 같은 데이터 타입 정보를 가지고 있어야 합니다.

DataFrame은 '비타입형'이며 Dataset은 '타입형'입니다.
물론 DataFrame에도 데이터 타입이 있습니다. 하지만 스키마에 명시된 데이터 타입의 일치 여부를 런타임이 되어서야 확인합니다.
반면에 Dataset은 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인합니다.

## UDF
UDF는 레코드별로 데이터를 처리하는 함수입니다.
UDF는 기본적으로 특정 SparkSession이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록됩니다.
스칼라, 파이썬, 자바로 UDF를 개발할 수 있습니다. 하지만 언어별로 성능에 영향을 미칠 수 있으므로 주의해야 합니다.

스파크는 드라이버에서 함수를 직렬화하고 네트워크를 통해 모든 익스큐터 프로세스로 전달합니다.
이 과정은 사용하는 언어와 관계없이 발생합니다.
스칼라나 자바로 함수를 작성했다면 JVM 환경에서만 사용할 수 있습니다.
스파크 제공하는 코드 생성 기능을 활용할 수 없어 약간의 성능 저하가 발생합니다.

파이썬으로 함수를 작성했다면 매우 다르게 동작합니다.
스파크는 워커 노드에 파이썬 프로세스를 실행하고 파이썬이 이해할 수 있는 포맷으로 모든 데이터를 직렬화(JVM 사용 언어에도 존재)합니다.
그리고 파이썬 프로세스에 있는 데이터의 로우마다 함수를 실행하고 마지막으로 JVM과 스파크에 처리 결과를 반환합니다.

파이썬 프로세스를 시작하는 부하도 크지만, 진짜 부하는 파이썬으로 데이터를 전달하기 위해 직렬화하는 과정에서 발생합니다.
첫째, 직렬화에 큰 부하가 발생합니다.
둘째, 데이터가 파이썬으로 전달되면 스파크에서 워커 메모리를 관리할 수 없습니다.
그러므로 JVM과 파이썬이 동일한 머신에서 메모리 경합을 하면 자원에 제약이 생겨 워커가 비정상적으로 종료될 가능성이 있습니다.
자바나 스칼라로 사용자 정의 함수를 작성하는 것이 좋습니다.

스파크는 자체 데이터 타입을 사용하기 때문에 함수를 정의할 때 반환 타입을 지정하는 것이 좋습니다.
함수에서 반환될 실제 데이터 타입과 일치하지 않는 데이터 타입을 지정하면 스파크는 오류가 아닌 null 값을 반환합니다.

```scala
  spark.udf.register("power3", power3(_:Double):Double)
  udfExampleDF.selectExpr("power3(num)").show(2)
```
```bash
  # create the jar using SBT
  sbt clean assembly

  # Pass the jar to the PySpark session
  pyspark --jars [path/to/jar/x.jar]
```

```python
  from pyspark.sql.types import BooleanType
  spark.udf.registerJavaFunction("my_udf", "com.example.spark.udf.UDF", BooleanType())

  # Use your UDF!
  spark.sql("""SELECT my_udf('test')""").show()
```
https://medium.com/wbaa/using-scala-udfs-in-pyspark-b70033dd69b9
https://github.com/johnmuller87/spark-udf

##  Hive UDF
하이브 문법을 사용해서 만든 UDF도 SparkSession을 생성할 때 SparkSession.builder().enableHiveSupport()를 명시해 하이브 지원 기능을 활성화해서 사용할 수 있습니다.
사전에 컴파일된 스칼라와 자바 패키지에서만 지원되므로 라이브러리 의존성을 명시해야 합니다.

# Execution
구조적 API로 작성된 코드는 스파크가 논리적 실행 계획으로 변환합니다.
스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인합니다.
스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행합니다.

카탈리스트 옵티마이저(Catalyst Optimizer)는 코드를 넘겨받고 실제 실행 계획을 생성합니다.
스파크는 실행계획을 수행한 수 결과를 반환합니다.

## Logical Execution Plan
드라이버나 익스큐터의 정보를 고려하지 않고 추상적 트랜스포메이션만 표현합니다.
그리고 사용자의 다양한 표현식을 최적화된 버전으로 변환합니다.

이 과정으로 사용자 코드는 검증 전 논리적 실행 계획(unresolved logical plan)으로 변환됩니다.
코드의 유효성과 테이블이나 컬럼의 존재 여부만을 판단하는 과정이므로 아직 실행 계획을 검증하지 않은 상태입니다.

스파크 분석기(analyzer)는 컬럼과 테이블을 검증하기 위해 카탈로그, 모든 테이블의 저장소 그리고 DataFrame 정보를 활용합니다.
필요한 테이블이나 컬럼이 카탈로그에 없다면 검증 전 논리적 실행 계획이 만들어지지 않습니다.
테이블과 컬럼에 대한 검증 결과는 카탈리스트 옵티마이저로 전달됩니다.

## Pysical Execution Plan
스파크 실행 계획이라고도 불리는 물리적 실행 계획은 논리적 실행 계획을 클러스터 환경에서 실행하는 방법을 정의합니다.
다양한 물리적 실행 전략을 생성하고 비용 모델을 이용해서 비교한 후 최적의 전략을 선택합니다.
물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환됩니다.
스파크는 DataFrame, Dataset, SQL로 정의된 쿼리를 RDD 트랜스포메이션으로 컴파일합니다.

## Catalyst optimizer
스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트(Catalyst) 엔진을 사용합니다.
카탈리스트 엔진은 논리적 실행 계획을 최적화하는 다양한 규칙의 모음입니다.
스파크는 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블을 가지고 있습니다.

https://www.youtube.com/watch?v=GDeePbbCz2g
https://towardsdatascience.com/apache-spark-3-0-adaptive-query-execution-2359e87ae31f
https://databricks.com/kr/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html

## Tungsten
스파크의 IO 최적화와 네트워크 및 디스크 하드웨어 그리고 데이터 포맷(parquet, binary format)의 향상됨에 따라 CPU가 bottleneck이 되었다.
Tungsten 프로젝트는 DataFrame이 메모리와 CPU의 효율성을 하드웨어의 한계치에 가깝도록 향상시키는 최적화된 내부 포맷을 사용하게 만드는 프로젝트이다.
스파크의 최적화된 내부 포맷을 사용하면 스파크가 지원하는 어떤 언어 API를 사용하더라도 동일한 효과와 효율성을 얻을 수 있습니다.

Tungsten 프로젝트에서는 명시적 메모리를 관리로 JVM 객체와 GC의 overhead를 줄입니다.
Java 객체 overhead를 살펴보면 "abcd"라는 문자열이 있을 떄 Native는 4bytes인 반면 Java String은 header와 hash를 포함해 48bytes로 상당한 차이가 있다.
메모리 관리로 인해 데이터 구조 관리가 가능하게 되어 CPU cache 효율성이 최적화 합니다.
binary data에 직접 명령하는 byte code를 generation하는 컴파일러를 추가 했습니다.

https://www.youtube.com/watch?v=5ajs8EIPWGI

## AQE : Adaptive Query Engine
<!-- TODO -->
## DPP : Dynamic partition pruning
<!-- TODO -->